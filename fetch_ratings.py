import requests
import json
import time
import re
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from statistics import mean
import csv
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

try:
    from tqdm import tqdm
except ImportError:
    print("æç¤º: å®‰è£… tqdm åº“ä»¥è·å¾—è¿›åº¦æ¡æ˜¾ç¤º: pip install tqdm")
    def tqdm(iterable, **kwargs):
        return iterable

# API é…ç½®
API_BASE_URL = "https://api2.openreview.net/notes"
LIMIT = 1000  # æ¯é¡µè¯„è®ºæ•°é‡
MAX_WORKERS = 50  # æœ€å¤§çº¿ç¨‹æ•°
INITIAL_DELAY = 0.1  # åˆå§‹è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰

# é‡è¯•é…ç½®
MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_BACKOFF_FACTOR = 0.5  # é€€é¿å› å­
RETRY_STATUS_FORCELIST = [500, 502, 503, 504, 429]  # éœ€è¦é‡è¯•çš„çŠ¶æ€ç 

# è¯·æ±‚å¤´
HEADERS = {
    "Accept": "application/json,text/*;q=0.99",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
    "Referer": "https://openreview.net/",
    "Origin": "https://openreview.net"
}

@dataclass
class PaperRatingInfo:
    """è®ºæ–‡è¯„åˆ†ä¿¡æ¯"""
    paper_id: str
    ratings: List[int]
    min_rating: Optional[int]
    max_rating: Optional[int]
    avg_rating: Optional[float]
    reviewer_count: int

class RatingExtractor:
    """ä»è¯„è®ºä¸­æå–è¯„åˆ†çš„å·¥å…·ç±»"""

    # è¯„åˆ†æ¨¡å¼
    RATING_PATTERNS = [
        r'Rating:\s*(\d+)',
        r'è¯„åˆ†[:ï¼š]\s*(\d+)',
        r'rating[:ï¼š]\s*(\d+)',
        r'(\d+)\s*/\s*10',
        r'(\d+)\s*out\s*of\s*10',
        r'Overall\s*Rating[:ï¼š]\s*(\d+)',
        r'Overall[:ï¼š]\s*(\d+)',
        r'Recommendation[:ï¼š]\s*(\d+)',
    ]

    @staticmethod
    def extract_rating_from_text(text: str) -> Optional[int]:
        """ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†"""
        if not text:
            return None

        for pattern in RatingExtractor.RATING_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                try:
                    rating = int(matches[0])
                    # ç¡®ä¿è¯„åˆ†åœ¨åˆç†èŒƒå›´å†… (1-10)
                    if 1 <= rating <= 10:
                        return rating
                except ValueError:
                    continue

        # ç‰¹æ®Šå¤„ç†: ç›´æ¥æŸ¥æ‰¾ 1-10 çš„æ•°å­—
        numbers = re.findall(r'\b([1-9]|10)\b', text)
        if numbers and len(numbers) == 1:
            try:
                return int(numbers[0])
            except ValueError:
                pass

        return None

class ICLR26RatingCrawler:
    """ICLR 2026 è¯„è®ºå’Œè¯„åˆ†çˆ¬è™«"""

    def __init__(self, max_workers: int = MAX_WORKERS, delay: float = INITIAL_DELAY):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            delay: APIè¯·æ±‚å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_workers = max_workers
        self.delay = delay
        self.rating_extractor = RatingExtractor()
        self.lock = threading.Lock()
        self.results = []
        self.failed_papers = []

        # è®¾ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=RETRY_BACKOFF_FACTOR,
            status_forcelist=RETRY_STATUS_FORCELIST,
            allowed_methods=["GET"]
        )

        # åˆ›å»ºä¼šè¯å¹¶é…ç½®é‡è¯•
        self.session = requests.Session()
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def fetch_paper_comments(self, paper_id: str) -> Optional[Dict]:
        """
        è·å–è®ºæ–‡çš„æ‰€æœ‰è¯„è®ºï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            paper_id: è®ºæ–‡ID

        Returns:
            APIå“åº”æ•°æ®æˆ–None
        """
        params = {
            "count": "true",
            "details": "writable,signatures,invitation,presentation,tags",
            "domain": "ICLR.cc/2026/Conference",
            "forum": paper_id,
            "limit": LIMIT,
            "trash": "true"
        }

        url = f"{API_BASE_URL}?{requests.compat.urlencode(params)}"

        try:
            # ä½¿ç”¨é…ç½®äº†é‡è¯•çš„ä¼šè¯
            response = self.session.get(url, headers=HEADERS, timeout=30)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            print(f"âŒ è·å–è®ºæ–‡ {paper_id} è¯„è®ºå¤±è´¥: {e}")
            return None
        except json.JSONDecodeError as e:
            print(f"âŒ è§£æè®ºæ–‡ {paper_id} å“åº”JSONå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ è·å–è®ºæ–‡ {paper_id} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return None

    def extract_ratings_from_comments(self, comments_data: Dict) -> List[int]:
        """
        ä»è¯„è®ºæ•°æ®ä¸­æå–è¯„åˆ†

        Args:
            comments_data: è¯„è®ºæ•°æ®

        Returns:
            è¯„åˆ†åˆ—è¡¨
        """
        ratings = []
        notes = comments_data.get("notes", [])

        for note in notes:
            # è·å–è¯„è®ºå†…å®¹
            content = note.get("content", {})

            # ç›´æ¥æå–ratingå­—æ®µ (æ ‡å‡†ICLRè¯„åˆ†æ ¼å¼)
            rating_field = content.get("rating", {})
            if isinstance(rating_field, dict):
                rating_value = rating_field.get("value")
            else:
                rating_value = rating_field

            # ICLRè¯„åˆ†æ˜¯0-10çš„æ•´æ•°
            if rating_value is not None and isinstance(rating_value, int) and 0 <= rating_value <= 10:
                ratings.append(rating_value)

        return ratings

    def process_single_paper(self, paper_id: str, max_retries: int = 3) -> Optional[PaperRatingInfo]:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            paper_id: è®ºæ–‡ID
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            è®ºæ–‡è¯„åˆ†ä¿¡æ¯æˆ–None
        """
        for attempt in range(max_retries):
            try:
                # è·å–è¯„è®ºæ•°æ®
                comments_data = self.fetch_paper_comments(paper_id)
                if not comments_data:
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) * 0.5  # æŒ‡æ•°é€€é¿
                        print(f"âš ï¸  è®ºæ–‡ {paper_id} ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                    else:
                        return None

                # æå–è¯„åˆ†
                ratings = self.extract_ratings_from_comments(comments_data)

                if not ratings:
                    return PaperRatingInfo(
                        paper_id=paper_id,
                        ratings=[],
                        min_rating=None,
                        max_rating=None,
                        avg_rating=None,
                        reviewer_count=0
                    )

                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                return PaperRatingInfo(
                    paper_id=paper_id,
                    ratings=ratings,
                    min_rating=min(ratings),
                    max_rating=max(ratings),
                    avg_rating=round(mean(ratings), 2),
                    reviewer_count=len(ratings)
                )

            except Exception as e:
                print(f"âŒ å¤„ç†è®ºæ–‡ {paper_id} ç¬¬ {attempt + 1} æ¬¡å°è¯•æ—¶å‡ºé”™: {e}")
                if attempt < max_retries - 1:
                    wait_time = (2 ** attempt) * 0.5  # æŒ‡æ•°é€€é¿
                    print(f"   ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    print(f"   è®ºæ–‡ {paper_id} å¤„ç†å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    return None

        return None

    def process_papers_batch(self, paper_ids: List[str]) -> List[PaperRatingInfo]:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡ï¼ˆå¤šçº¿ç¨‹ï¼‰

        Args:
            paper_ids: è®ºæ–‡IDåˆ—è¡¨

        Returns:
            è®ºæ–‡è¯„åˆ†ä¿¡æ¯åˆ—è¡¨
        """
        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_paper = {
                executor.submit(self.process_single_paper, paper_id): paper_id
                for paper_id in paper_ids
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in tqdm(as_completed(future_to_paper), total=len(paper_ids), desc="å¤„ç†è¿›åº¦"):
                paper_id = future_to_paper[future]
                try:
                    result = future.result()
                    if result:
                        results.append(result)
                    else:
                        self.failed_papers.append(paper_id)
                except Exception as e:
                    print(f"âŒ å¤„ç†è®ºæ–‡ {paper_id} æ—¶å‡ºé”™: {e}")
                    self.failed_papers.append(paper_id)

                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                time.sleep(self.delay)

        return results

    def save_results(self, results: List[PaperRatingInfo], output_file: str = "iclr26_ratings.json"):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: è®ºæ–‡è¯„åˆ†ä¿¡æ¯åˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶å
        """
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data = []
        for result in results:
            data.append({
                "paper_id": result.paper_id,
                "ratings": result.ratings,
                "min_rating": result.min_rating,
                "max_rating": result.max_rating,
                "avg_rating": result.avg_rating,
                "reviewer_count": result.reviewer_count
            })

        # ä¿å­˜ä¸ºJSON
        try:
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ JSON æ–‡ä»¶å·²ä¿å­˜: {output_file}")
            print(f"   æ€»è®¡: {len(results)} ç¯‡è®ºæ–‡çš„è¯„åˆ†ä¿¡æ¯")
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")

        # ä¿å­˜ä¸ºCSV
        csv_file = output_file.replace(".json", ".csv")
        try:
            with open(csv_file, "w", newline="", encoding="utf-8") as f:
                fieldnames = ["paper_id", "ratings", "min_rating", "max_rating", "avg_rating", "reviewer_count"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()

                for item in data:
                    row = item.copy()
                    row["ratings"] = "; ".join(map(str, row["ratings"]))
                    writer.writerow(row)

            print(f"ğŸ’¾ CSV æ–‡ä»¶å·²ä¿å­˜: {csv_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")

    def load_paper_ids_from_json(self, json_file: str) -> List[str]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½è®ºæ–‡IDåˆ—è¡¨

        Args:
            json_file: JSONæ–‡ä»¶è·¯å¾„

        Returns:
            è®ºæ–‡IDåˆ—è¡¨
        """
        try:
            with open(json_file, "r", encoding="utf-8") as f:
                papers = json.load(f)

            paper_ids = []
            for paper in papers:
                paper_id = paper.get("paper_id")
                if paper_id:
                    paper_ids.append(paper_id)

            print(f"ğŸ“„ ä» {json_file} åŠ è½½äº† {len(paper_ids)} ä¸ªè®ºæ–‡ID")
            return paper_ids

        except Exception as e:
            print(f"âŒ åŠ è½½è®ºæ–‡IDå¤±è´¥: {e}")
            return []

    def run(self, paper_ids: List[str], output_file: str = "iclr26_ratings.json"):
        """
        è¿è¡Œçˆ¬è™«

        Args:
            paper_ids: è®ºæ–‡IDåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶å
        """
        if not paper_ids:
            print("âš ï¸ æ²¡æœ‰è®ºæ–‡IDéœ€è¦å¤„ç†")
            return

        print("=" * 60)
        print(" ICLR 2026 Ratings Crawler")
        print("=" * 60)
        print(f"å¾…å¤„ç†è®ºæ–‡æ•°é‡: {len(paper_ids)}")
        print(f"çº¿ç¨‹æ•°: {self.max_workers}")
        print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
        print("-" * 60)
        print("â³ å¼€å§‹è·å–è¯„è®ºå’Œè¯„åˆ†...")

        # å¤„ç†è®ºæ–‡
        start_time = time.time()
        results = self.process_papers_batch(paper_ids)
        end_time = time.time()

        print("-" * 60)
        print(f"âœ… å¤„ç†å®Œæˆ!")
        print(f"   - æˆåŠŸ: {len(results)} ç¯‡è®ºæ–‡")
        print(f"   - å¤±è´¥: {len(self.failed_papers)} ç¯‡è®ºæ–‡")
        print(f"   - è€—æ—¶: {end_time - start_time:.2f} ç§’")

        if results:
            # ä¿å­˜ç»“æœ
            self.save_results(results, output_file)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.show_statistics(results)

        if self.failed_papers:
            print(f"âš ï¸  å¤±è´¥çš„è®ºæ–‡ID: {self.failed_papers[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ª

    def show_statistics(self, results: List[PaperRatingInfo]):
        """
        æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

        Args:
            results: è®ºæ–‡è¯„åˆ†ä¿¡æ¯åˆ—è¡¨
        """
        print("-" * 60)
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")

        # æœ‰è¯„åˆ†çš„è®ºæ–‡æ•°é‡
        papers_with_ratings = [r for r in results if r.ratings]
        print(f"   - æœ‰è¯„åˆ†çš„è®ºæ–‡: {len(papers_with_ratings)} / {len(results)}")

        if papers_with_ratings:
            # è¯„åˆ†åˆ†å¸ƒ
            all_ratings = []
            for result in papers_with_ratings:
                all_ratings.extend(result.ratings)

            if all_ratings:
                print(f"   - æ€»è¯„åˆ†æ•°é‡: {len(all_ratings)}")
                print(f"   - å¹³å‡è¯„åˆ†: {mean(all_ratings):.2f}")
                print(f"   - æœ€ä½è¯„åˆ†: {min(all_ratings)}")
                print(f"   - æœ€é«˜è¯„åˆ†: {max(all_ratings)}")

                # è¯„åˆ†åˆ†å¸ƒ
                rating_counts = {}
                for rating in all_ratings:
                    rating_counts[rating] = rating_counts.get(rating, 0) + 1

                print(f"   - è¯„åˆ†åˆ†å¸ƒ:")
                for rating in sorted(rating_counts.keys()):
                    count = rating_counts[rating]
                    percentage = (count / len(all_ratings)) * 100
                    print(f"     {rating}åˆ†: {count} ({percentage:.1f}%)")

def main():
    """ä¸»å‡½æ•°"""
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = ICLR26RatingCrawler(max_workers=MAX_WORKERS, delay=INITIAL_DELAY)

        # ä»å·²æœ‰çš„è®ºæ–‡æ•°æ®æ–‡ä»¶åŠ è½½è®ºæ–‡ID
        paper_ids = crawler.load_paper_ids_from_json("iclr26_all_papers.json")

        if not paper_ids:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡IDï¼Œè¯·å…ˆè¿è¡Œ request_iclr26.py è·å–è®ºæ–‡æ•°æ®")
            return 1

        # è¿è¡Œçˆ¬è™«
        crawler.run(paper_ids, "iclr26_ratings.json")

        print("=" * 60)
        print("âœ¨ æ‰€æœ‰ä»»åŠ¡å®Œæˆ!")
        print("=" * 60)

    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return 1
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return 1

    return 0

if __name__ == "__main__":
    exit(main())